{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate simulation likelihood given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import integrate, stats\n",
    "from scipy.special import expit, binom\n",
    "\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "from datetime import datetime\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import cloudpickle\n",
    "import dask\n",
    "import distributed\n",
    "from dask.distributed import Client\n",
    "import itertools\n",
    "\n",
    "import pymc3\n",
    "\n",
    "import inspect\n",
    "from collections import OrderedDict\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "import functools\n",
    "\n",
    "# Plotly \n",
    "import cufflinks as cf\n",
    "import plotly\n",
    "from plotly.offline import iplot as plt\n",
    "from plotly import graph_objs as plt_type\n",
    "from plotly import graph_objs as go\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "\n",
    "Distributed dask workers seem very fiddly at importing non-standard modules, so it's better to just copy-paste the functions into this notebook sadly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get relatve age-related risks, we have to first re-group into our basic age groups, \n",
    "# then devide by total population (here's a non-well-defined subset of UK, so absolute values wont be used, only relative)\n",
    "import numpy as np\n",
    "import inspect\n",
    "from collections import OrderedDict\n",
    "\n",
    "def regroup_by_age(inp, fromAgeSplits, toAgeSplits, maxAge=100., maxAgeWeight = 5.):\n",
    "    fromAgeSplits = np.concatenate([np.array([0]), fromAgeSplits, np.array([maxAge])]) # Add a zero at beginning for calculations\n",
    "    toAgeSplits = np.concatenate([np.array([0]), toAgeSplits, np.array([maxAge])]) # Add inf at end for calculations\n",
    "    def getOverlap(a, b):\n",
    "        return max(0, min(a[1], b[1]) - max(a[0], b[0]))\n",
    "    out = np.zeros((len(toAgeSplits)-1,)+inp.shape[1:])\n",
    "    for from_ind in range(1, len(fromAgeSplits)):\n",
    "        # Redistribute to the new bins by calculating how many years in from_ind-1:from_ind falls into each output bin\n",
    "        cur_out_distribution = (\n",
    "        [getOverlap(toAgeSplits[cur_to_ind-1:cur_to_ind+1],fromAgeSplits[from_ind-1:from_ind+1])  for cur_to_ind in range(1, len(toAgeSplits))]\n",
    "        )\n",
    "        \n",
    "        if cur_out_distribution[-1] > 0:\n",
    "            cur_out_distribution[-1] = maxAgeWeight # Define the relative number of ages if we have to distribute between second to last and last age groups\n",
    "\n",
    "        cur_out_distribution = cur_out_distribution/np.sum(cur_out_distribution)\n",
    "        \n",
    "        for to_ind in range(len(out)):\n",
    "            out[to_ind] += cur_out_distribution[to_ind] * inp[from_ind-1]\n",
    "            \n",
    "    return out\n",
    "\n",
    "\n",
    "# PARAMETER DICTIONARIES AND TABLES\n",
    "# -----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def build_paramDict(cur_func):\n",
    "    \"\"\"\n",
    "    This function iterates through all inputs of a function, \n",
    "    and saves the default argument names and values into a dictionary.\n",
    "    \n",
    "    If any of the default arguments are functions themselves, then recursively (depth-first) adds an extra field to\n",
    "    the dictionary, named <funcName + \"_params\">, that contains its inputs and arguments.\n",
    "    \n",
    "    The output of this function can then be passed as a \"kwargs\" object to the highest level function, \n",
    "    which will then pass the parameter values to the lower dictionary levels appropriately\n",
    "    \"\"\"\n",
    "    \n",
    "    paramDict = OrderedDict()\n",
    "    \n",
    "    allArgs = inspect.getfullargspec(cur_func)\n",
    "    \n",
    "    # Check if there are any default parameters, if no, just return empty dict\n",
    "    if allArgs.defaults is None:\n",
    "        return paramDict\n",
    "    \n",
    "    \n",
    "    for argname, argval in zip(allArgs.args[-len(allArgs.defaults):], allArgs.defaults):\n",
    "        # Save the default argument\n",
    "        paramDict[argname] = argval\n",
    "        # If the default argument is a function, inspect it for further \n",
    "        \n",
    "        if callable(argval):\n",
    "            # print(argname)\n",
    "            paramDict[argname+\"_params\"] = build_paramDict(argval)\n",
    "\n",
    "    return paramDict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Do a mapping between dictionary and parameter table row (for convenient use)\n",
    "\n",
    "# Flatten the dictionary into a table with a single row (but many headers):\n",
    "def paramDict_toTable(paramDict):\n",
    "    paramTable = pd.DataFrame()\n",
    "    def paramDictRecurseIter(cur_table, cur_dict, preString):\n",
    "        # Iterate through the dictionary to find all keys not ending in \"_params\", \n",
    "        # and add them to the table with name <preString + key>\n",
    "        # \n",
    "        # If the key doesn end in \"_params\", then append the key to preString, in call this function on the value (that is a dict)\n",
    "        for key, value in cur_dict.items():\n",
    "            if key.endswith(\"_params\"):\n",
    "                paramDictRecurseIter(cur_table, value, preString+key+\"_\")\n",
    "            else:\n",
    "                paramTable[preString+key] = [value]\n",
    "                \n",
    "        # For the rare case where we want to keep an empty dictionary, the above for cycle doesn't keep it\n",
    "        if len(cur_dict)==0:\n",
    "            paramTable[preString] = [OrderedDict()]\n",
    "                \n",
    "        return cur_table\n",
    "    \n",
    "    return paramDictRecurseIter(paramTable, paramDict, preString=\"\")\n",
    "\n",
    "# Example dict -> table\n",
    "# paramTable_default = paramDict_toTable(paramDict_default)\n",
    "    \n",
    "\n",
    "def paramTable_toDict(paramTable, defaultDict=None):\n",
    "    # enable to pass a default dict (if paramTable is incomplete), in which we'll just add / overwrite the values\n",
    "    paramDict = defaultDict if defaultDict is not None else OrderedDict() \n",
    "    def placeArgInDictRecurse(argName, argVal, cur_dict):\n",
    "        # Find all \"_params_\" in the argName, and for each step more and more down in the dictionary\n",
    "        strloc = argName.find(\"_params_\")\n",
    "        if strloc == -1:\n",
    "            # We're at the correct level of dictionary\n",
    "            cur_dict[argName] = argVal\n",
    "            return cur_dict\n",
    "        else:\n",
    "            # step to the next level of dictionary\n",
    "            nextKey = argName[:strloc+len(\"_params_\")-1]\n",
    "            nextArgName = argName[strloc+len(\"_params_\"):]\n",
    "            if not nextKey in cur_dict:\n",
    "                cur_dict[nextKey] = OrderedDict()\n",
    "            placeArgInDictRecurse(nextArgName, argVal, cur_dict[nextKey])\n",
    "            \n",
    "        return cur_dict\n",
    "            \n",
    "    for key in paramTable.columns:\n",
    "        paramDict = placeArgInDictRecurse(key, paramTable.at[0,key], paramDict)\n",
    "        \n",
    "    return paramDict\n",
    "\n",
    "# Example table -> dict \n",
    "# paramDict_new = paramTable_toDict(paramTable_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONST_DATA_CUTOFF_DATE = \"20200414\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NHS England deaths dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NHS daily deaths report (about 24 hours behind)\n",
    "# TODO manually update link and column numbers (maybe not consistent across days, cannot yet automate)\n",
    "df_UK_NHS_daily_COVID_deaths = pd.read_excel(\n",
    "    \"https://www.england.nhs.uk/statistics/wp-content/uploads/sites/2/2020/04/COVID-19-total-announced-deaths-22-April-2020.xlsx\",\n",
    "    sheet_name = \"COVID19 total deaths by age\",\n",
    "    index_col=0,\n",
    "    usecols = \"B,E:AX\",\n",
    "    skip_rows = range(17),\n",
    "    nrows = 22\n",
    ").iloc[14:].transpose().set_index(\"Age group\").rename_axis(index = \"Date\", columns = \"AgeGroup\")\n",
    "\n",
    "df_UK_NHS_daily_COVID_deaths.index = pd.to_datetime(df_UK_NHS_daily_COVID_deaths.index, format=\"%Y-%m-%d\")\n",
    "\n",
    "df_UK_NHS_daily_COVID_deaths = df_UK_NHS_daily_COVID_deaths.drop(df_UK_NHS_daily_COVID_deaths.columns[:2], axis=1)\n",
    "\n",
    "df_UK_NHS_daily_COVID_deaths\n",
    "\n",
    "# Ignore very recent unreliable data points\n",
    "df_UK_NHS_daily_COVID_deaths = df_UK_NHS_daily_COVID_deaths.loc[\n",
    "            df_UK_NHS_daily_COVID_deaths.index <= CONST_DATA_CUTOFF_DATE]\n",
    "\n",
    "df_UK_NHS_daily_COVID_deaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NHS England CHESS - COVID hospitalisations - dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the aggregate data (ask @SebastianVollmer for aggregation details!)\n",
    "df_CHESS = pd.read_csv(\"/mnt/efs/data/CHESS_Aggregate20200417.csv\").drop(0)\n",
    "\n",
    "# Clean the dates and make them index\n",
    "# The \"1899-12-30\" is simply total, ignore that.\n",
    "# The 2020-09-03, 2020-10-03, 2020-11-03, 2020-12-03 are parsed wrong and are march 09-12 supposedly.\n",
    "# The data collection is only officially started across england on 09 March, the February dates seem empty, delete.\n",
    "# Rest are ok\n",
    "\n",
    "df_CHESS.index = pd.to_datetime(df_CHESS[\"DateOfAdmission\"].values,format=\"%d-%m-%Y\")\n",
    "\n",
    "# Ignore too old and too recent data points\n",
    "df_CHESS = df_CHESS.sort_index().drop(\"DateOfAdmission\", axis=1).query('20200309 <= index <= ' + CONST_DATA_CUTOFF_DATE)\n",
    "\n",
    "df_CHESS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CHESS.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the hospitalised people who tested positive for COVID, using cumsum (TODO: for now assuming they're all still in hospital)\n",
    "df_CHESS_newCOVID = df_CHESS.loc[:,df_CHESS.columns.str.startswith(\"AllAdmittedPatientsWithNewLabConfirmedCOVID19\")]\n",
    "\n",
    "df_CHESS_newCOVID.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an example simulation:\n",
    "simExample = np.load(\"/mnt/efs/results/run_20200408T195337/outTensor_20200408T195337_slr7ep10hy0q9iyr3k36.npy\")\n",
    "simExample_newOnly = np.load(\"/mnt/efs/results/run_20200408T195337/outTensor_20200408T195337_slr7ep10hy0q9iyr3k36_newOnly.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We expect \n",
    "def joinDataAndSimCurves(\n",
    "    df_curves, # a pandas dataframe with dates as index, and each column is a curve\n",
    "    simCurves, # curves x time np array\n",
    "    simStartDate, # curves start dates\n",
    "    simCurvesNames = None,\n",
    "    fulljoin = False # if true, then one keeps all dates in the simulation, instead of just the ones in the date \n",
    "    ):\n",
    "    \n",
    "    out_df = copy.deepcopy(df_curves)\n",
    "    \n",
    "    simCurveIndex = pd.date_range(start=simStartDate, freq='D', periods=simCurves.shape[1])\n",
    "    \n",
    "    if simCurvesNames is None:\n",
    "        simCurvesNames = [\"simCurve_{}\".format(i) for i in range(simCurves.shape[0])]\n",
    "    \n",
    "    join_type = \"outer\" if fulljoin else \"left\"\n",
    "    \n",
    "    for i, curve in enumerate(simCurves):\n",
    "        out_df = out_df.join(\n",
    "            pd.DataFrame(\n",
    "                index = simCurveIndex,\n",
    "                data = simCurves[i],\n",
    "                columns=[simCurvesNames[i]]\n",
    "            ),\n",
    "            how = join_type\n",
    "        )\n",
    "    \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy lik function to plot curves without data, get correct projection via functools.partial(..., projFunc = ...)\n",
    "def likFunc_dummyForPlotting(\n",
    "    sim, \n",
    "    simStartDate, \n",
    "    df = None,\n",
    "    sumAges = True,\n",
    "    outputDataframe = False, # If true, outputs the data-curves and simulated curves instead of likelihood, for plotting\n",
    "    fulljoin = False, # if true, then one keeps all dates in the simulation, instead of just the ones in the date\n",
    "    projFunc = lambda sim: np.diff(np.sum(sim[:,-1,:,:,:],axis=(1,2)),-1) # pass a lambda function to create a simCurve, this example does daily new deaths\n",
    "    ):\n",
    "    \n",
    "    \n",
    "    simCurves = projFunc(sim)\n",
    "    if sumAges:\n",
    "        simCurves = simCurves.sum(0, keepdims=True)\n",
    "    \n",
    "    if df is None:\n",
    "        df = pd.DataFrame(index= pd.date_range(start=simStartDate, freq='D', periods=simCurves.shape[1])) # an empty pandas dataframe with dates as index\n",
    "    else:\n",
    "        if sumAges:\n",
    "            df = pd.DataFrame(df.sum(1))\n",
    "            \n",
    "    # Join the two dataframes to align in time\n",
    "    df_full = joinDataAndSimCurves(\n",
    "        df_curves = df,\n",
    "        simCurves = simCurves, # curves x time np array\n",
    "        simStartDate = simStartDate, # curves start dates\n",
    "        fulljoin = fulljoin\n",
    "    )\n",
    "    \n",
    "    # If true, outputs the data-curves and simulated curves instead of likelihood, for plotting\n",
    "    if outputDataframe:\n",
    "        return df_full\n",
    "    \n",
    "    return None\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNegBinomParams(mu, alpha):\n",
    "    \"\"\" \n",
    "    From https://stats.stackexchange.com/questions/260580/negative-binomial-distribution-with-python-scipy-stats\n",
    "    Convert mean/dispersion parameterization of a negative binomial to the ones scipy supports\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mu : float \n",
    "       Mean of NB distribution.\n",
    "    alpha : float\n",
    "       Overdispersion parameter used for variance calculation.\n",
    "\n",
    "    See https://en.wikipedia.org/wiki/Negative_binomial_distribution#Alternative_formulations\n",
    "    \"\"\"\n",
    "    var = mu + alpha * mu ** 2\n",
    "    p = (var - mu) / var\n",
    "    r = mu ** 2 / (var - mu)\n",
    "    return r, p\n",
    "\n",
    "def NegBinom_logpmf(a, m, x):\n",
    "    binom_vec = np.array([binom(x1 + a - 1, x1) for x1 in x])\n",
    "    logpmf = np.log(binom_vec) + a * np.log(a / (m + a))  + x * np.log((m / (m + a)))\n",
    "    return logpmf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likFunc_deaths(\n",
    "    sim, # use newOnly for deaths by day needed here\n",
    "    simStartDate, \n",
    "    df,\n",
    "    sumAges = True,\n",
    "    outputDataframe = False, # If true, outputs the data-curves and simulated curves instead of likelihood, for plotting\n",
    "    fulljoin = False # if true, then one keeps all dates in the simulation, instead of just the ones in the date\n",
    "    ):\n",
    "\n",
    "        \n",
    "    # Get deaths by day in simulation in hospitals for people with positive tests\n",
    "    deaths_Sim_byAge = np.sum(sim[:,-1,2,:,:],axis=(1))\n",
    "    \n",
    "    \n",
    "    if sumAges:\n",
    "        deaths_Sim = np.sum(deaths_Sim_byAge,axis=0, keepdims=True) \n",
    "        deaths_data = pd.DataFrame(df.sum(1))\n",
    "    else:\n",
    "        # Change the grouping of ages to be same as dataset\n",
    "        deaths_Sim_byAge_regroup = regroup_by_age(\n",
    "            deaths_Sim_byAge, \n",
    "            fromAgeSplits = np.arange(10,80+1,10), \n",
    "            toAgeSplits = np.arange(20,80+1,20)\n",
    "        )\n",
    "        \n",
    "        deaths_Sim = deaths_Sim_byAge_regroup\n",
    "        deaths_data = df\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Join the two dataframes to align in time\n",
    "    df_full = joinDataAndSimCurves(\n",
    "        df_curves = deaths_data, # a pandas dataframe with dates as index, and each column is a curve\n",
    "        simCurves = deaths_Sim, # curves x time np array\n",
    "        simStartDate = simStartDate, # curves start dates\n",
    "        fulljoin = fulljoin\n",
    "    )\n",
    "    \n",
    "    # If true, outputs the data-curves and simulated curves instead of likelihood, for plotting\n",
    "    if outputDataframe:\n",
    "        return df_full\n",
    "    \n",
    "    # We assume the order of columns in data and in simCurves are the same!\n",
    "    #return df_full\n",
    "    return np.nansum(\n",
    "        NegBinom_logpmf(2., \n",
    "                        # Select all simCurve columns and reshape to a single vector\n",
    "                        m = 1e-8+np.reshape(df_full.loc[:,df_full.columns.str.startswith(\"simCurve_\")==True].values,-1), \n",
    "                        # Select all data columns and reshape to a single vector\n",
    "                        x = np.reshape(df_full.loc[:,(df_full.columns.str.startswith(\"simCurve_\")==True)==False].values,-1)\n",
    "                       )\n",
    "    )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likFunc_deaths(\n",
    "    sim = simExample, \n",
    "    simStartDate = '2020-03-12',\n",
    "    df = copy.deepcopy(df_UK_NHS_daily_COVID_deaths),\n",
    "    sumAges=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likFunc_newHospPositive(\n",
    "    sim, # Here we'll make sure to pass the \"_newOnly\" tensor!\n",
    "    simStartDate, \n",
    "    df,\n",
    "    sumAges = True,\n",
    "    outputDataframe = False, # If true, outputs the data-curves and simulated curves instead of likelihood, for plotting\n",
    "    fulljoin = False # if true, then one keeps all dates in the simulation, instead of just the ones in the date\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Get the number of hospitalised people testing positive each day.\n",
    "    This fits well with \"policyFunc_testing_symptomaticOnly\" being active, which prioratises testing hospitalised people\n",
    "    As per 09 April this is a very reasonable assumption\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the simulation curves of hospitalised people getting positive tests each day\n",
    "    # TODO - run the simulation with actual test numbers each day, would make fits a LOT better.\n",
    "    # Take into account the number of positive tested people who leave the hospital and add that as well \n",
    "    # (as they were also replaced by new people testing positive in the hospital!)\n",
    "    \n",
    "    # Change in hospital and testing positive\n",
    "    newHospPositives_sim = np.sum(sim[:,:,2,1,:], axis=(1,))\n",
    "    \n",
    "    if sumAges:\n",
    "        hospPos_Sim = np.sum(newHospPositives_sim,axis=0, keepdims=True) \n",
    "        hospPos_data = pd.DataFrame(df.sum(1))\n",
    "    else:\n",
    "        \n",
    "        # Change the grouping of ages to be same as dataset\n",
    "        \n",
    "        hospPos_Sim = regroup_by_age(\n",
    "            newHospPositives_sim, \n",
    "            fromAgeSplits = np.arange(10,80+1,10), \n",
    "            toAgeSplits = np.concatenate([np.array([1,5,15,25]),np.arange(45,85+1,10)])\n",
    "        )\n",
    "        \n",
    "        hospPos_data = df\n",
    "        \n",
    "        \n",
    "    # Join the two dataframes to align in time\n",
    "    df_full = joinDataAndSimCurves(\n",
    "        df_curves = hospPos_data, # a pandas dataframe with dates as index, and each column is a curve\n",
    "        simCurves = hospPos_Sim, # curves x time np array\n",
    "        simStartDate = simStartDate, # curves start dates\n",
    "        fulljoin = fulljoin\n",
    "    )\n",
    "    \n",
    "    # If true, outputs the data-curves and simulated curves instead of likelihood, for plotting\n",
    "    if outputDataframe:\n",
    "        return df_full\n",
    "    \n",
    "    \n",
    "    # We assume the order of columns in data and in simCurves are the same!\n",
    "    #return df_full\n",
    "    return np.nansum(\n",
    "        NegBinom_logpmf(2., \n",
    "                        # Select all simCurve columns and reshape to a single vector\n",
    "                        m = 1e-8+np.reshape(df_full.loc[:,df_full.columns.str.startswith(\"simCurve_\")==True].values,-1), \n",
    "                        # Select all data columns and reshape to a single vector\n",
    "                        x = np.reshape(df_full.loc[:,(df_full.columns.str.startswith(\"simCurve_\")==True)==False].values,-1)\n",
    "                       )\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likFunc_newHospPositive(\n",
    "    sim = simExample_newOnly, \n",
    "    simStartDate = '2020-03-12',\n",
    "    df = copy.deepcopy(\n",
    "        df_CHESS_newCOVID\n",
    "    ),\n",
    "    sumAges=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load simulations and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loadDir = \"/mnt/efs/results/run_20200411T134739/\" # <- large base ensemble of random simulations\n",
    "#loadDir = \"/mnt/efs/results/run_20200414T210254/\" # <- GP based ensemble\n",
    "loadDir = \"/mnt/efs/results/run_20200421T002117/\" # <- NEW - GP based ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while True:\n",
    "    print(\"Loading {}\".format(i))\n",
    "    try:\n",
    "        if i == 0:\n",
    "            paramTable_loaded_dd = dd.from_pandas(pd.read_hdf(loadDir + 'paramTable_part0', key=\"paramTable\"),chunksize=1000)\n",
    "        else:\n",
    "            paramTable_loaded_dd = paramTable_loaded_dd.append(pd.read_hdf(loadDir + 'paramTable_part{}'.format(i), key=\"paramTable\"))\n",
    "        i += 1\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "len(paramTable_loaded_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(paramTable_loaded_dd[[\"likelihood_0\", \"likelihood_1\", \"likelihood_2\", \"likelihood_3\"]]\n",
    " .compute()\n",
    " .rename(columns={\n",
    "     \"likelihood_0\": \"likDailyTotalDeaths\", \n",
    "     \"likelihood_1\": \"likDailyTotalDeaths_byAgeGroups\", \n",
    "     \"likelihood_2\": \"likDailyNewPosTestsHospital\",\n",
    "     \"likelihood_3\": \"likDailyNewPosTestsHospital_byAgeGroups\"\n",
    " })\n",
    ").iplot(subplots=True, subplot_titles=True, kind=\"histogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on likelihoods (see above histograms for value choices)\n",
    "paramTable_goodOnly = (paramTable_loaded_dd\n",
    "    #.query(\"likelihood_1 > -650\")\n",
    "    .query(\"likelihood_0 > -260 & likelihood_2 > -280\")\n",
    "    #.query(\"likelihood_total > -3000\")\n",
    "    #.query(\"likelihood_0 > -200 & likelihood_2 > -250\")\n",
    "    #.query(\"likelihood_0 > -140 & likelihood_1 > -350 & likelihood_2 > -200 & likelihood_3 > -1300\")\n",
    " #.query(\"likelihood_1 > -400 & likelihood_3 > -2100\")\n",
    " .compute()\n",
    "                      )\n",
    "\n",
    "print(len(paramTable_goodOnly))\n",
    "paramTable_goodOnly.sort_values(\"likelihood_total\").tail(30).loc[:,\"likelihood_0\":\"likelihood_total\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper funcs for organising data and simulations for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tryLoad(\n",
    "    fn,\n",
    "    shape = (9,8,4,3,100,),\n",
    "    fill = 0.\n",
    "    ):\n",
    "    try:\n",
    "        return np.load(fn)\n",
    "    except:\n",
    "        return fill*np.ones(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinSimulations(\n",
    "    loadDir,\n",
    "    paramTable,\n",
    "    simRowIndices, # from paramTables filled with bestStartTime column\n",
    "    likFunc,\n",
    "    simSuffix,\n",
    "    df, # data\n",
    "    paramTable_startTimeColname = \"bestStartTime\",\n",
    "    sumAges = True\n",
    "    ):\n",
    "    \n",
    "    # Get the individual data frames that contain the appropriate projection(s) of the simulation to match the data\n",
    "    all_df_full = [\n",
    "        likFunc(\n",
    "            sim = tryLoad(loadDir + paramTable.at[ind,\"out_fname\"][:-4] + simSuffix + \".npy\"), \n",
    "            simStartDate = paramTable.at[ind, paramTable_startTimeColname],\n",
    "            df = df,\n",
    "            sumAges=sumAges,\n",
    "            outputDataframe=True,\n",
    "            fulljoin=True\n",
    "        )\n",
    "        \n",
    "        for ind in simRowIndices\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    df_data_cols = all_df_full[0].loc[:,(all_df_full[0].columns.str.startswith(\"simCurve_\")==True)==False]\n",
    "    \n",
    "    origSimColNames = all_df_full[0].loc[:,(all_df_full[0].columns.str.startswith(\"simCurve_\")==True)].columns\n",
    "   \n",
    "    \n",
    "    df_out = df_data_cols.join(\n",
    "        # Get only the simCurve columns from the dataframes and rename them to include the simulation row index\n",
    "        other = [\n",
    "            cur_df.loc[:,cur_df.columns.str.startswith(\"simCurve_\")==True].rename(\n",
    "                columns = {s: \"row_\" + str(rowInd) + \"_\" + s for s in origSimColNames}\n",
    "            )\n",
    "            \n",
    "            for cur_df, rowInd in zip(all_df_full, simRowIndices)\n",
    "        ],\n",
    "        \n",
    "        how = \"outer\"\n",
    "    )\n",
    "    \n",
    "    return df_out\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot simulations against data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(Y, X=None, now = True, **kwargs):\n",
    "    \"\"\"\n",
    "    plots a matrix Y (n x num_lines)\n",
    "    with optionally x spacing\n",
    "    \"\"\"\n",
    "    plots = list()\n",
    "    \n",
    "    if len(Y.shape)==1:\n",
    "        Y = Y[:,np.newaxis]\n",
    "    \n",
    "    if X is None:\n",
    "        X = np.arange(Y.shape[0])\n",
    "    \n",
    "    for num_line in range(Y.shape[1]):\n",
    "        plots.append(\n",
    "            plt_type.Scatter(\n",
    "                x = X[:,num_line] if len(X.shape)==2 else X,\n",
    "                y = Y[:,num_line],\n",
    "                **kwargs\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    if now:\n",
    "        plt(plots)\n",
    "    else:\n",
    "        return plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot new deaths per day\n",
    "tmp = joinSimulations(\n",
    "    loadDir = loadDir,\n",
    "    paramTable=paramTable_goodOnly,\n",
    "    simRowIndices = paramTable_goodOnly.sort_values(\"likelihood_0\").tail(20).index,\n",
    "    paramTable_startTimeColname = \"realStartDate\",\n",
    "    likFunc = likFunc_deaths,\n",
    "    simSuffix=\"_newOnly\",\n",
    "    df = df_UK_NHS_daily_COVID_deaths,\n",
    "    sumAges=True\n",
    ")#.iplot(line=dict(color='firebrick', width=4))\n",
    "\n",
    "\n",
    "a = plot(tmp.values, tmp.index, now=False)\n",
    "a[0][\"marker\"]=dict(color=\"black\", size=8)\n",
    "a[0][\"mode\"]=\"markers\"\n",
    "for ind in range(1,len(a)):\n",
    "    a[ind][\"opacity\"]=1./np.sqrt(len(a)/2.)\n",
    "plt(go.Figure(data=a, layout=go.Layout(title=\"Daily in-hospital total COVID deaths in the UK\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot new hospitalisations per day\n",
    "tmp = joinSimulations(\n",
    "    loadDir = loadDir,\n",
    "    paramTable=paramTable_goodOnly,\n",
    "    simRowIndices = paramTable_goodOnly.sort_values(\"likelihood_total\").tail(200).index,\n",
    "    paramTable_startTimeColname = \"realStartDate\",\n",
    "    likFunc = likFunc_newHospPositive,\n",
    "    simSuffix=\"_newOnly\",\n",
    "    df = df_CHESS_newCOVID,\n",
    "    sumAges=True\n",
    ")\n",
    "\n",
    "a = plot(tmp.values, tmp.index, now=False)\n",
    "a[0][\"marker\"]=dict(color=\"black\", size=8)\n",
    "a[0][\"mode\"]=\"markers\"\n",
    "for ind in range(1,len(a)):\n",
    "    a[ind][\"opacity\"]=1./np.sqrt(len(a)/2.)\n",
    "plt(go.Figure(data=a, layout=go.Layout(title=\"Daily in-hospital positive tests in the UK\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot new infections per day\n",
    "tmp = joinSimulations(\n",
    "    loadDir = loadDir,\n",
    "    paramTable=paramTable_goodOnly,\n",
    "    simRowIndices = paramTable_goodOnly.sort_values(\"likelihood_total\").tail(200).index,\n",
    "    paramTable_startTimeColname = \"realStartDate\",\n",
    "    likFunc = functools.partial(\n",
    "        likFunc_dummyForPlotting, \n",
    "        projFunc= lambda sim: sim[:,1].sum(2).sum(1)\n",
    "        ),\n",
    "    simSuffix=\"_newOnly\",\n",
    "    df = None,\n",
    "    sumAges=True\n",
    ")\n",
    "\n",
    "a = plot(tmp.values, tmp.index, now=False)\n",
    "for ind in range(len(a)):\n",
    "    a[ind][\"opacity\"]=1./np.sqrt(len(a)/2.)\n",
    "\n",
    "plt(go.Figure(data=a, layout=go.Layout(title=\"Daily new infections in the UK [no data available]\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py-corona-2-ensemble]",
   "language": "python",
   "name": "conda-env-.conda-py-corona-2-ensemble-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
